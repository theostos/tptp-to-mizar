model_name: Qwen/Qwen2.5-Coder-32B-Instruct
model_config: Qwen25Config32B
nodes: 8
ntasks_per_node: 4
gpus_per_node: 4
strategy:
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 2
  context_parallel_size: 4
  sequence_parallel: True

trainer:
  devices: auto
  accelerator: gpu
  accumulate_grad_batches: 1
  max_steps: 2800
  log_every_n_steps: 1
  limit_val_batches: 0
  val_check_interval: 0
  num_sanity_val_steps: 0

logger:
  save_last: true
  every_n_train_steps: 200
  monitor: reduced_train_loss
  save_top_k: 3
  save_on_train_epoch_end: true
  save_optim_on_train_end: true

optimizer:
  optimizer: adam
  lr: 0.00001
  adam_beta1: 0.9
  adam_beta2: 0.95
  weight_decay: 0.0001
  use_distributed_optimizer: true
  clip_grad: 1.0
  bf16: true

scheduler:
  warmup_steps: 140
  max_steps: 2800
  constant_steps: 0
  min_lr: 0.0

datamodule:
  dataset_preprocess_filepath: "export/dataset/training.jsonl"
  seq_length: 32768
  micro_batch_size: 1
  global_batch_size: 16
  num_workers: 8